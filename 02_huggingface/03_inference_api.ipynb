{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InferenceAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**질문답변**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/deepset/roberta-base-squad2\"\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": {\n",
    "\t\"question\": \"What is my name?\",\n",
    "\t\"context\": \"My name is Clara and I live in Berkeley.\"\n",
    "},\n",
    "})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text로 Image 생성하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-dev\"\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.content\n",
    "image_bytes = query({\n",
    "\t\"inputs\": \"Dog on the Moon\",\n",
    "})\n",
    "# You can access the image with PIL.Image for example\n",
    "import io\n",
    "from PIL import Image\n",
    "image = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**한글 입력해서 영어 이미지 생성 모델 추론하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install protobuf\n",
    "\n",
    "pip install diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자가 입력한 값\n",
    "text = '인공지능을 공부하는 학생들'\n",
    "\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "# from huggingface_hub import login\n",
    "# login(token = 'Bearer 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')\n",
    "\n",
    "\n",
    "\n",
    "article_ko = \"인공지능을 공부하는 학생들\"\n",
    "\n",
    "# 번역 모델 불러오기\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "# 번역 모델에서 사용할 토크나이저 가져오기\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "\n",
    "# 전처리\n",
    "# translate Hindi to French\n",
    "tokenizer.src_lang = \"ko_KR\"\n",
    "\n",
    "# 한글 토크나이징\n",
    "encoded_ko = tokenizer(article_ko, return_tensors=\"pt\")\n",
    "\n",
    "# 추론\n",
    "generated_tokens = model.generate(\n",
    "    **encoded_ko,\n",
    "    forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"]\n",
    ")\n",
    "\n",
    "print(generated_tokens)\n",
    "\n",
    "# 후처리\n",
    "# 번역 결과 decoding(자연어로 변경)\n",
    "translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "print(translated_text)\n",
    "\n",
    "# text to image 모델 가져오기\n",
    "image_creator = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "# GPU연산이 부족할때 CPU로 연산을 할 수 있게 해주는 transformers의 기능\n",
    "# GPU가 충분하면 사용하지 않아도된다.\n",
    "image_creator.enable_model_cpu_offload() \n",
    "\n",
    "# image 생성 프롬프트 (번역된 결과로 진행)\n",
    "prompt = translated_text\n",
    "\n",
    "image = image_creator(\n",
    "    prompt,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    guidance_scale=3.5,\n",
    "    num_inference_steps=50,\n",
    "    max_sequence_length=512,\n",
    "    generator=torch.Generator(\"cpu\").manual_seed(0)\n",
    ").images[0]\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "input_text = \"코끼리를 타고 있는 원숭이\"\n",
    "\n",
    "# 모델 불러오기\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-ko-en\")\n",
    "\n",
    "# 추론, 토큰화 \n",
    "result = translator(input_text)\n",
    "\n",
    "translated_text = result[0]['translation_text']\n",
    "\n",
    "# 번역 결과\n",
    "translated_text\n",
    "\n",
    "# text to image\n",
    "API_URL = \"https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-dev\"\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "# infrence-api 호출\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.content\n",
    "image_bytes = query({\n",
    "\t\"inputs\": translated_text,\n",
    "})\n",
    "# 결과출력\n",
    "import io\n",
    "from PIL import Image\n",
    "image = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
